


背景

目標検出
コンピュータビジョン分野における重要なタスクです。オープンセット目標検出（Open-set Object Detection）とクローズドセット目標検出（Closed-set Object Detection）は、目標検出分野における2つの概念です。

クローズドセット目標検出
* **定義**：モデルが訓練時とテスト時に遭遇するすべてのオブジェクトクラスが既知であると仮定し、つまり可能なすべてのオブジェクトクラスが訓練段階で定義・アノテーション済みであること
* **目標**：既知クラスのオブジェクトを正確に識別し、位置を特定すること
* **課題**：既知クラスの精密な分類と位置特定、さらにクラス間の微細な差異の処理が主な課題
* **代表的なネットワーク構造**：Faster-RCNN、SSD、YOLO、DETR

オープンセット目標検出
* **定義**：モデルがテスト時に訓練段階で未知の新しいクラス（訓練データに出現していないクラス）に遭遇する可能性があると仮定
* **目標**：既知クラスのオブジェクトの識別・位置特定に加え、未知クラスのオブジェクトも識別・処理できること
* **課題**：未知クラスへの汎化能力を持ち、新しいクラスに遭遇した際に単純に既知クラスに分類するのではなく、適切な応答ができること
* **代表的なネットワーク構造**：OWL-ViT、Grounding DINO

総じて、YOLOとOWL-ViTはそれぞれ利点があり、異なる用途に適しています。YOLOは速度とリアルタイム性能に重点を置き、OWL-ViTはオープン語彙と長尾分布データセットにおいてより強い汎化能力を示します。

本記事では、オープンセット目標検出の代表作であるOWL-ViTの技術特性と、エッジチップAX650N上での展開性能について概観します。

OWL-ViT
2022年の論文「Simple Open-Vocabulary Object Detection with Vision Transformers」に由来し、著者はGoogle ResearchのMatthias Mindererらです。論文は主に、Vision Transformerベースの画像-テキストモデルをオープン語彙（open-vocabulary）目標検出タスクに転移する方法を研究しています。

論文リンク：2205.06230
Githubプロジェクト：github.com/google-research/scenic/projects/owl_vit

オープン語彙目標検出とは、モデルが訓練データで見たことのないクラスを識別・位置特定できることを指します。このタスクは、訓練データが比較的少ない長尾分布とオープン語彙設定において特に重要です。

主な貢献
* 画像レベルの事前学習をオープン語彙目標検出タスクに転移する、シンプルかつ強力な手法を提案。標準的なVision Transformerアーキテクチャを最小限の修正で使用し、対比的な画像-テキスト事前学習とエンドツーエンドの目標検出微調整により実現
* ゼロショットとワンショット画像条件付き目標検出において最先端の性能を達成
   * ゼロショットは、モデルが訓練過程で見たことのないクラスをテスト時に識別できること
   * ワンショットは、1つのクエリ画像断片のみに基づいて新しいオブジェクトを検出すること
* 設計の有効性を示す詳細なスケーリングと切除研究を提供。モデルサイズ、事前学習期間、モデルアーキテクチャが検出性能に与える影響を分析し、モデルサイズと事前学習時間の増加が継続的に検出性能を向上させることを発見



![v2-f3e28db9a7b4b35f7df6a2aeae881d79_1440w](https://github.com/user-attachments/assets/3a01c1ca-f471-4c81-9f1b-a969dbc910f0)

2つのフェーズ

1. まず、大規模な画像-テキストデータを使用して、画像とテキストエンコーダーを対照的に事前学習を行います：
* 画像とテキストの対応関係を学習
* 意味空間での表現を獲得

2. 次に、検出ヘッドを追加し、二分マッチング損失を用いて中規模の目標検出データセット上で微調整を行います。モデルは異なる方式でクエリを実行でき、オープン語彙または少数ショット画像条件付き目標検出を行うことができます。

![v2-53f977081b989d538eb547a5fe14e996_1440w](https://github.com/user-attachments/assets/ddbdfbfd-7e80-473c-9878-dfefc14e39ea)

![v2-47886e96e682c798985c1f6a66c4283f_1440w](https://github.com/user-attachments/assets/b1ff81fa-6c7f-444e-81d2-2d7529c9e33e)

そうですね。この論文は、シンプルかつスケーラブルなオープン語彙目標検出手法を提案しています。対照的な事前学習と適切な微調整戦略により、限られたアノテーションデータでも強力な目標検出性能を実現できます。この手法は今後の研究のための強力なベースラインを提供し、さまざまなフレームワークで容易に実装することができます。

DEMOパッケージの説明

ファイル名と説明：
- main_ax650：AX650Nベースのデモ、NPU計算
- main_x86：x86 LinuxベースのデモCPU計算
- owlvit-image.axmodel：画像エンコーディングモデル（AX650N NPU用）
- owlvit-image.onnx：画像エンコーディングモデル（CPU用）
- owlvit-text.onnx：テキストエンコーディングモデル
- owlvit-post.onnx：目標検出後処理モデル

ボード上での実行例：

```bash
/opt/test/owlvit # ./main --ienc owlvit-image.axmodel --tenc owlvit-text.onnx -d owlvit-post.onnx -v vocab.txt -i ssd_horse.jpg -t "horse" --thread 8
[実行ログ...]
horse 191.756058 55.418949 229.225601 318.581055

/opt/test/owlvit # ./main --ienc owlvit-image.axmodel --tenc owlvit-text.onnx -d owlvit-post.onnx -v vocab.txt -i ssd_horse.jpg -t text.txt --thread 8
[実行ログ...]
a photo of person 268.899292 20.153463 88.163696 235.837906
a photo of person 428.696014 123.745819 19.836823 55.102310
horse 191.756058 55.418949 229.225601 318.581055
a photo of car 0.000000 98.398750 145.470108 92.571877
a photo of dog 145.470108 203.093140 57.306412 156.490570
```

上記から分かるように、AX650NのNPU上でのimage encoderの実行に22ms、単一Promptの検出後処理に14msかかっています。テキスト特徴量は以前のCLIPの実装経験から事前に特徴ベクトルを抽出できるため、実際のビジネスでは25 FPS以上のフレームレートを実現できます。これは基本バージョンのリアルタイム処理要件を満たしています。

入力テキスト：
```text
/opt/test/owlvit # cat text.txt
a photo of person
horse
a photo of car
a photo of dog
```

入力画像：[画像]

![v2-54bd2d5f63ac73ad82f1db7303354313_1440w](https://github.com/user-attachments/assets/f95dc135-5814-4755-b0d3-bed1140413ae)



終わりに

Vision Transformerネットワークモデルの急速な発展に伴い、より多くの興味深いAIアプリケーションがクラウドサービスからエッジデバイスやエンドデバイスへと徐々に移行していくでしょう。私たちがすでに共有したTransformerネットワーク構造に基づく最先端の人気モデルの適応成果は以下の通りです：

* テキストによる画像検索（CLIP）
* 画像修復（SAM+LaMa）
* DINOv2
* SegFormer
* EfficientViT
* DETR
* Swin Transformer

新年にはエンドデバイスへの展開に適したTransformerネットワークがさらに増えることでしょう。

謝辞
* OWL-ViTの適応とDEMO開発：@折秋水
* Kimi Chat - より広い世界を見せてくれる

私は圈圈虫、技術を愛する元ネット有名人の**中年おじさん**です。AXERA技術交流QQグループ（139953715）にぜひご参加ください。業界の多くの専門家がオンラインで質問に答えています。また、AXera-Pi Pro（AX650N搭載）の最新情報もフォローしてください。

それぞれのコメントに対する回答を整理してみましょう：

虫おじさんへのコメント「666（素晴らしい）、推論速度がかなり速いですね」
→ ありがとうございます。性能を重視して最適化しました。

紫炎さんの質問「owlvit-image.axmodelはどのように変換したのでしょうか。owlv2を変換しようとしてpulsar2を使用していますが、quant.axmodel export successの後にkilledになってしまいます」
→ これは技術的な問題で、より詳細な情報が必要かもしれません。メモリ使用量や変換プロセスの具体的なログがあると、より正確な診断ができるでしょう。

阿晨3さんの質問「なぜGithubの encode text Inference Cost time が0.190662sなのに、ここでは0.050662sになっているのでしょうか。どのような最適化で何倍も速くなったのでしょうか」

圈圈虫の回答「説明を更新しました」「入力テキストが単一のプロンプトの場合、処理時間はわずか14msです」
→ 単一プロンプトと複数プロンプトで処理時間に大きな違いがあることを説明されています。






